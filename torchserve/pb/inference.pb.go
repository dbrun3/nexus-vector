// Code generated by protoc-gen-go. DO NOT EDIT.
// versions:
// 	protoc-gen-go v1.36.8
// 	protoc        v5.29.3
// source: inference.proto

package pb

import (
	status "google.golang.org/genproto/googleapis/rpc/status"
	protoreflect "google.golang.org/protobuf/reflect/protoreflect"
	protoimpl "google.golang.org/protobuf/runtime/protoimpl"
	emptypb "google.golang.org/protobuf/types/known/emptypb"
	reflect "reflect"
	sync "sync"
	unsafe "unsafe"
)

const (
	// Verify that this generated code is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(20 - protoimpl.MinVersion)
	// Verify that runtime/protoimpl is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(protoimpl.MaxVersion - 20)
)

type PredictionsRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Name of model.
	ModelName string `protobuf:"bytes,1,opt,name=model_name,json=modelName,proto3" json:"model_name,omitempty"` //required
	// Version of model to run prediction on.
	ModelVersion string `protobuf:"bytes,2,opt,name=model_version,json=modelVersion,proto3" json:"model_version,omitempty"` //optional
	// Input data for model prediction
	Input map[string][]byte `protobuf:"bytes,3,rep,name=input,proto3" json:"input,omitempty" protobuf_key:"bytes,1,opt,name=key" protobuf_val:"bytes,2,opt,name=value"` //required
	// SequenceId is required for StreamPredictions2 API.
	SequenceId    *string `protobuf:"bytes,4,opt,name=sequence_id,json=sequenceId,proto3,oneof" json:"sequence_id,omitempty"` //optional
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *PredictionsRequest) Reset() {
	*x = PredictionsRequest{}
	mi := &file_inference_proto_msgTypes[0]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *PredictionsRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*PredictionsRequest) ProtoMessage() {}

func (x *PredictionsRequest) ProtoReflect() protoreflect.Message {
	mi := &file_inference_proto_msgTypes[0]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use PredictionsRequest.ProtoReflect.Descriptor instead.
func (*PredictionsRequest) Descriptor() ([]byte, []int) {
	return file_inference_proto_rawDescGZIP(), []int{0}
}

func (x *PredictionsRequest) GetModelName() string {
	if x != nil {
		return x.ModelName
	}
	return ""
}

func (x *PredictionsRequest) GetModelVersion() string {
	if x != nil {
		return x.ModelVersion
	}
	return ""
}

func (x *PredictionsRequest) GetInput() map[string][]byte {
	if x != nil {
		return x.Input
	}
	return nil
}

func (x *PredictionsRequest) GetSequenceId() string {
	if x != nil && x.SequenceId != nil {
		return *x.SequenceId
	}
	return ""
}

type PredictionResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Response content for prediction
	Prediction []byte `protobuf:"bytes,1,opt,name=prediction,proto3" json:"prediction,omitempty"`
	// SequenceId is required for StreamPredictions2 API.
	SequenceId *string `protobuf:"bytes,2,opt,name=sequence_id,json=sequenceId,proto3,oneof" json:"sequence_id,omitempty"` //optional
	// Error information for StreamPredictions2 API.
	Status        *status.Status `protobuf:"bytes,3,opt,name=status,proto3,oneof" json:"status,omitempty"` //optional
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *PredictionResponse) Reset() {
	*x = PredictionResponse{}
	mi := &file_inference_proto_msgTypes[1]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *PredictionResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*PredictionResponse) ProtoMessage() {}

func (x *PredictionResponse) ProtoReflect() protoreflect.Message {
	mi := &file_inference_proto_msgTypes[1]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use PredictionResponse.ProtoReflect.Descriptor instead.
func (*PredictionResponse) Descriptor() ([]byte, []int) {
	return file_inference_proto_rawDescGZIP(), []int{1}
}

func (x *PredictionResponse) GetPrediction() []byte {
	if x != nil {
		return x.Prediction
	}
	return nil
}

func (x *PredictionResponse) GetSequenceId() string {
	if x != nil && x.SequenceId != nil {
		return *x.SequenceId
	}
	return ""
}

func (x *PredictionResponse) GetStatus() *status.Status {
	if x != nil {
		return x.Status
	}
	return nil
}

type TorchServeHealthResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// TorchServe health
	Health        string `protobuf:"bytes,1,opt,name=health,proto3" json:"health,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *TorchServeHealthResponse) Reset() {
	*x = TorchServeHealthResponse{}
	mi := &file_inference_proto_msgTypes[2]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *TorchServeHealthResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*TorchServeHealthResponse) ProtoMessage() {}

func (x *TorchServeHealthResponse) ProtoReflect() protoreflect.Message {
	mi := &file_inference_proto_msgTypes[2]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use TorchServeHealthResponse.ProtoReflect.Descriptor instead.
func (*TorchServeHealthResponse) Descriptor() ([]byte, []int) {
	return file_inference_proto_rawDescGZIP(), []int{2}
}

func (x *TorchServeHealthResponse) GetHealth() string {
	if x != nil {
		return x.Health
	}
	return ""
}

var File_inference_proto protoreflect.FileDescriptor

const file_inference_proto_rawDesc = "" +
	"\n" +
	"\x0finference.proto\x12 org.pytorch.serve.grpc.inference\x1a\x1bgoogle/protobuf/empty.proto\x1a\x17google/rpc/status.proto\"\x9f\x02\n" +
	"\x12PredictionsRequest\x12\x1d\n" +
	"\n" +
	"model_name\x18\x01 \x01(\tR\tmodelName\x12#\n" +
	"\rmodel_version\x18\x02 \x01(\tR\fmodelVersion\x12U\n" +
	"\x05input\x18\x03 \x03(\v2?.org.pytorch.serve.grpc.inference.PredictionsRequest.InputEntryR\x05input\x12$\n" +
	"\vsequence_id\x18\x04 \x01(\tH\x00R\n" +
	"sequenceId\x88\x01\x01\x1a8\n" +
	"\n" +
	"InputEntry\x12\x10\n" +
	"\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n" +
	"\x05value\x18\x02 \x01(\fR\x05value:\x028\x01B\x0e\n" +
	"\f_sequence_id\"\xa6\x01\n" +
	"\x12PredictionResponse\x12\x1e\n" +
	"\n" +
	"prediction\x18\x01 \x01(\fR\n" +
	"prediction\x12$\n" +
	"\vsequence_id\x18\x02 \x01(\tH\x00R\n" +
	"sequenceId\x88\x01\x01\x12/\n" +
	"\x06status\x18\x03 \x01(\v2\x12.google.rpc.StatusH\x01R\x06status\x88\x01\x01B\x0e\n" +
	"\f_sequence_idB\t\n" +
	"\a_status\"2\n" +
	"\x18TorchServeHealthResponse\x12\x16\n" +
	"\x06health\x18\x01 \x01(\tR\x06health2\x80\x04\n" +
	"\x14InferenceAPIsService\x12\\\n" +
	"\x04Ping\x12\x16.google.protobuf.Empty\x1a:.org.pytorch.serve.grpc.inference.TorchServeHealthResponse\"\x00\x12{\n" +
	"\vPredictions\x124.org.pytorch.serve.grpc.inference.PredictionsRequest\x1a4.org.pytorch.serve.grpc.inference.PredictionResponse\"\x00\x12\x83\x01\n" +
	"\x11StreamPredictions\x124.org.pytorch.serve.grpc.inference.PredictionsRequest\x1a4.org.pytorch.serve.grpc.inference.PredictionResponse\"\x000\x01\x12\x86\x01\n" +
	"\x12StreamPredictions2\x124.org.pytorch.serve.grpc.inference.PredictionsRequest\x1a4.org.pytorch.serve.grpc.inference.PredictionResponse\"\x00(\x010\x01B\x02P\x01b\x06proto3"

var (
	file_inference_proto_rawDescOnce sync.Once
	file_inference_proto_rawDescData []byte
)

func file_inference_proto_rawDescGZIP() []byte {
	file_inference_proto_rawDescOnce.Do(func() {
		file_inference_proto_rawDescData = protoimpl.X.CompressGZIP(unsafe.Slice(unsafe.StringData(file_inference_proto_rawDesc), len(file_inference_proto_rawDesc)))
	})
	return file_inference_proto_rawDescData
}

var file_inference_proto_msgTypes = make([]protoimpl.MessageInfo, 4)
var file_inference_proto_goTypes = []any{
	(*PredictionsRequest)(nil),       // 0: org.pytorch.serve.grpc.inference.PredictionsRequest
	(*PredictionResponse)(nil),       // 1: org.pytorch.serve.grpc.inference.PredictionResponse
	(*TorchServeHealthResponse)(nil), // 2: org.pytorch.serve.grpc.inference.TorchServeHealthResponse
	nil,                              // 3: org.pytorch.serve.grpc.inference.PredictionsRequest.InputEntry
	(*status.Status)(nil),            // 4: google.rpc.Status
	(*emptypb.Empty)(nil),            // 5: google.protobuf.Empty
}
var file_inference_proto_depIdxs = []int32{
	3, // 0: org.pytorch.serve.grpc.inference.PredictionsRequest.input:type_name -> org.pytorch.serve.grpc.inference.PredictionsRequest.InputEntry
	4, // 1: org.pytorch.serve.grpc.inference.PredictionResponse.status:type_name -> google.rpc.Status
	5, // 2: org.pytorch.serve.grpc.inference.InferenceAPIsService.Ping:input_type -> google.protobuf.Empty
	0, // 3: org.pytorch.serve.grpc.inference.InferenceAPIsService.Predictions:input_type -> org.pytorch.serve.grpc.inference.PredictionsRequest
	0, // 4: org.pytorch.serve.grpc.inference.InferenceAPIsService.StreamPredictions:input_type -> org.pytorch.serve.grpc.inference.PredictionsRequest
	0, // 5: org.pytorch.serve.grpc.inference.InferenceAPIsService.StreamPredictions2:input_type -> org.pytorch.serve.grpc.inference.PredictionsRequest
	2, // 6: org.pytorch.serve.grpc.inference.InferenceAPIsService.Ping:output_type -> org.pytorch.serve.grpc.inference.TorchServeHealthResponse
	1, // 7: org.pytorch.serve.grpc.inference.InferenceAPIsService.Predictions:output_type -> org.pytorch.serve.grpc.inference.PredictionResponse
	1, // 8: org.pytorch.serve.grpc.inference.InferenceAPIsService.StreamPredictions:output_type -> org.pytorch.serve.grpc.inference.PredictionResponse
	1, // 9: org.pytorch.serve.grpc.inference.InferenceAPIsService.StreamPredictions2:output_type -> org.pytorch.serve.grpc.inference.PredictionResponse
	6, // [6:10] is the sub-list for method output_type
	2, // [2:6] is the sub-list for method input_type
	2, // [2:2] is the sub-list for extension type_name
	2, // [2:2] is the sub-list for extension extendee
	0, // [0:2] is the sub-list for field type_name
}

func init() { file_inference_proto_init() }
func file_inference_proto_init() {
	if File_inference_proto != nil {
		return
	}
	file_inference_proto_msgTypes[0].OneofWrappers = []any{}
	file_inference_proto_msgTypes[1].OneofWrappers = []any{}
	type x struct{}
	out := protoimpl.TypeBuilder{
		File: protoimpl.DescBuilder{
			GoPackagePath: reflect.TypeOf(x{}).PkgPath(),
			RawDescriptor: unsafe.Slice(unsafe.StringData(file_inference_proto_rawDesc), len(file_inference_proto_rawDesc)),
			NumEnums:      0,
			NumMessages:   4,
			NumExtensions: 0,
			NumServices:   1,
		},
		GoTypes:           file_inference_proto_goTypes,
		DependencyIndexes: file_inference_proto_depIdxs,
		MessageInfos:      file_inference_proto_msgTypes,
	}.Build()
	File_inference_proto = out.File
	file_inference_proto_goTypes = nil
	file_inference_proto_depIdxs = nil
}
